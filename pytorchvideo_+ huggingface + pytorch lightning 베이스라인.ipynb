{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5471bf37-978d-4b72-931e-bc5bab4b9225",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Collecting decord\n",
      "  Using cached decord-0.6.0-py3-none-win_amd64.whl (24.7 MB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from decord) (1.20.3)\n",
      "Installing collected packages: decord\n",
      "Successfully installed decord-0.6.0\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (1.25.11)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "     ------------------------------------- 825.8/825.8 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (4.4.0)\n",
      "Collecting lightning-utilities>=0.4.2\n",
      "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (21.0)\n",
      "Collecting fsspec[http]>2021.06.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.0/143.0 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (4.62.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (1.11.0+cu113)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorch-lightning) (1.20.3)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
      "     ------------------------------------- 517.2/517.2 kB 10.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.24.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     ------------------------------------- 323.5/323.5 kB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=17.1->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.25.11)\n",
      "Installing collected packages: multidict, fsspec, charset-normalizer, yarl, torchmetrics, lightning-utilities, aiohttp, pytorch-lightning\n",
      "Successfully installed aiohttp-3.8.3 charset-normalizer-2.1.1 fsspec-2023.1.0 lightning-utilities-0.6.0.post0 multidict-6.0.4 pytorch-lightning-1.9.0 torchmetrics-0.11.1 yarl-1.8.2\n",
      "Collecting pytorchvideo\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "     -------------------------------------- 132.7/132.7 kB 8.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting av\n",
      "  Downloading av-10.0.0-cp39-cp39-win_amd64.whl (25.3 MB)\n",
      "     --------------------------------------- 25.3/25.3 MB 11.1 MB/s eta 0:00:00\n",
      "Collecting parameterized\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting iopath\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "     ---------------------------------------- 42.2/42.2 kB 2.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pytorchvideo) (2.8.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (1.20.3)\n",
      "Collecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (4.62.3)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (2.2.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (9.1.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fvcore->pytorchvideo) (0.8.9)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from iopath->pytorchvideo) (4.4.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (301)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->fvcore->pytorchvideo) (0.4.4)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (setup.py): started\n",
      "  Building wheel for pytorchvideo (setup.py): finished with status 'done'\n",
      "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188712 sha256=dbf82550680334b1fa5b6d6f4bd8c9cc2627980ba3d21a9b2f9fdacd9ba38b23\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e0\\85\\41\\42153f93cd5715669b2fd7765b25cfca52c7277ad5a6762112\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61438 sha256=33a25416a46f99ba17b08620d57d94b70bfcee3e19522c7a22ca633616776fc4\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\83\\42\\02\\66178d16e5c44dc26d309931834956baeda371956e86fbd876\n",
      "  Building wheel for iopath (setup.py): started\n",
      "  Building wheel for iopath (setup.py): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31551 sha256=ad74bd0b9b11d2f59462297f4586dbdd8f722750ad4822298d57dffe3f0a846b\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\c1\\13\\6d\\441d8f2af76ee6d2a3e67eebb1d0c556fefcee0a8b32266a8e\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: parameterized, av, yacs, portalocker, iopath, fvcore, pytorchvideo\n",
      "Successfully installed av-10.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.8.1 portalocker-2.7.0 pytorchvideo-0.1.5 yacs-0.1.8\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "     ---------------------------------------- 89.4/89.4 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n",
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.2-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.7/106.7 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from segmentation-models-pytorch) (9.1.1)\n",
      "Collecting pretrainedmodels==0.7.4\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from segmentation-models-pytorch) (4.62.3)\n",
      "Collecting efficientnet-pytorch==0.7.1\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torchvision>=0.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from segmentation-models-pytorch) (0.12.0+cu113)\n",
      "Collecting timm==0.6.12\n",
      "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
      "     ------------------------------------- 549.1/549.1 kB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.0+cu113)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from timm==0.6.12->segmentation-models-pytorch) (0.10.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.20.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->segmentation-models-pytorch) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (21.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.9->huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (2.4.7)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n",
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16484 sha256=38c1c959581cb9a7d79b355849e898a515d5ee3b2232d2d57f69b04693804341\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\29\\16\\24\\752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
      "  Building wheel for pretrainedmodels (setup.py): started\n",
      "  Building wheel for pretrainedmodels (setup.py): finished with status 'done'\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=61037 sha256=ca9cac34e868fa578ea0530b73e5ec672e24772ca837ceae1356a3acdbe1a35d\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\d1\\3b\\4e\\2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.2 timm-0.6.12\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.6/41.6 kB ? eta 0:00:00\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.6.0\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install decord\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install pytorch-lightning\n",
    "!pip install pytorchvideo\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install transformers\n",
    "!pip install einops\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cac5a52-9eb9-4bb7-aa62-b7c419273eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/XuezheMax/apollo/blob/master/optim/apollo.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Apollo(Optimizer):\n",
    "    r\"\"\"Implements Atom algorithm.\n",
    "        Arguments:\n",
    "            params (iterable): iterable of parameters to optimize or dicts defining\n",
    "                parameter groups\n",
    "            lr (float): learning rate\n",
    "            beta (float, optional): coefficient used for computing running averages of gradient (default: 0.9)\n",
    "            eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-4)\n",
    "            rebound (str, optional): recified bound for diagonal hessian:\n",
    "                ``'constant'`` | ``'belief'`` (default: None)\n",
    "            warmup (int, optional): number of warmup steps (default: 500)\n",
    "            init_lr (float, optional): initial learning rate for warmup (default: lr/1000)\n",
    "            weight_decay (float, optional): weight decay coefficient (default: 0)\n",
    "            weight_decay_type (str, optional): type of weight decay:\n",
    "                ``'L2'`` | ``'decoupled'`` | ``'stable'`` (default: 'L2')\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, beta=0.9, eps=1e-4, rebound='constant', warmup=500, init_lr=None, weight_decay=0, weight_decay_type=None):\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate value: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= beta < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(beta))\n",
    "        if rebound not in ['constant', 'belief']:\n",
    "            raise ValueError(\"Invalid recitifed bound: {}\".format(rebound))\n",
    "        if not 0.0 <= warmup:\n",
    "            raise ValueError(\"Invalid warmup updates: {}\".format(warmup))\n",
    "        if init_lr is None:\n",
    "            init_lr = lr / 1000\n",
    "        if not 0.0 <= init_lr <= lr:\n",
    "            raise ValueError(\"Invalid initial learning rate: {}\".format(init_lr))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if weight_decay_type is None:\n",
    "            weight_decay_type = 'L2' if rebound == 'constant' else 'decoupled'\n",
    "        if weight_decay_type not in ['L2', 'decoupled', 'stable']:\n",
    "            raise ValueError(\"Invalid weight decay type: {}\".format(weight_decay_type))\n",
    "\n",
    "        defaults = dict(lr=lr, beta=beta, eps=eps, rebound=rebound,\n",
    "                        warmup=warmup, init_lr=init_lr, base_lr=lr,\n",
    "                        weight_decay=weight_decay, weight_decay_type=weight_decay_type)\n",
    "        super(Apollo, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Apollo, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg_grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['approx_hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Previous update direction\n",
    "                    state['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                # Calculate current lr\n",
    "                if state['step'] < group['warmup']:\n",
    "                    curr_lr = (group['base_lr'] - group['init_lr']) * state['step'] / group['warmup'] + group['init_lr']\n",
    "                else:\n",
    "                    curr_lr = group['lr']\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Atom does not support sparse gradients.')\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] == 'L2':\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "\n",
    "                beta = group['beta']\n",
    "                eps = group['eps']\n",
    "                exp_avg_grad = state['exp_avg_grad']\n",
    "                B = state['approx_hessian']\n",
    "                d_p = state['update']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction = 1 - beta ** state['step']\n",
    "                alpha = (1 - beta) / bias_correction\n",
    "\n",
    "                # calc the diff grad\n",
    "                delta_grad = grad - exp_avg_grad\n",
    "                if group['rebound'] == 'belief':\n",
    "                    rebound = delta_grad.norm(p=np.inf)\n",
    "                else:\n",
    "                    rebound = 0.01\n",
    "                    eps = eps / rebound\n",
    "\n",
    "                # Update the running average grad\n",
    "                exp_avg_grad.add_(delta_grad, alpha=alpha)\n",
    "\n",
    "                denom = d_p.norm(p=4).add(eps)\n",
    "                d_p.div_(denom)\n",
    "                v_sq = d_p.mul(d_p)\n",
    "                delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-alpha) - B.mul(v_sq).sum()\n",
    "\n",
    "                # Update B\n",
    "                B.addcmul_(v_sq, delta)\n",
    "\n",
    "                # calc direction of parameter updates\n",
    "                if group['rebound'] == 'belief':\n",
    "                    denom = torch.max(B.abs(), rebound).add_(eps / alpha)\n",
    "                else:\n",
    "                    denom = B.abs().clamp_(min=rebound)\n",
    "\n",
    "                d_p.copy_(exp_avg_grad.div(denom))\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] != 'L2':\n",
    "                    if group['weight_decay_type'] == 'stable':\n",
    "                        weight_decay = group['weight_decay'] / denom.mean().item()\n",
    "                    else:\n",
    "                        weight_decay = group['weight_decay']\n",
    "                    d_p.add_(p, alpha=weight_decay)\n",
    "\n",
    "                p.add_(d_p, alpha=-curr_lr)\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8e4a2b-70dd-4bcc-9eca-6af9b217e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/issamemari/pytorch-multilabel-balanced-sampler/blob/master/sampler.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class MultilabelBalancedRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
    "    number of classes n_classes, samples from the data with equal probability per class\n",
    "    effectively oversampling minority classes and undersampling majority classes at the\n",
    "    same time. Note that using this sampler does not guarantee that the distribution of\n",
    "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
    "    sampling is based on a single class. This does however guarantee that all classes\n",
    "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
    "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
    "            of indices to sample only from\n",
    "            class_choice: a string indicating how class will be selected for every\n",
    "            sample:\n",
    "                \"least_sampled\": class with the least number of sampled labels so far\n",
    "                \"random\": class is chosen uniformly at random\n",
    "                \"cycle\": the sampler cycles through the classes sequentially\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        if self.indices is None:\n",
    "            self.indices = range(len(labels))\n",
    "\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "\n",
    "        # List of lists of example indices per class\n",
    "        self.class_indices = []\n",
    "        for class_ in range(self.num_classes):\n",
    "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
    "            lst = lst[np.isin(lst, self.indices)]\n",
    "            self.class_indices.append(lst)\n",
    "\n",
    "        self.counts = [0] * self.num_classes\n",
    "\n",
    "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
    "        self.class_choice = class_choice\n",
    "        self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        self.count += 1\n",
    "        return self.sample()\n",
    "\n",
    "    def sample(self):\n",
    "        class_ = self.get_class()\n",
    "        class_indices = self.class_indices[class_]\n",
    "        chosen_index = np.random.choice(class_indices)\n",
    "        if self.class_choice == \"least_sampled\":\n",
    "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
    "                if indicator == 1:\n",
    "                    self.counts[class_] += 1\n",
    "        return chosen_index\n",
    "\n",
    "    def get_class(self):\n",
    "        if self.class_choice == \"random\":\n",
    "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
    "        elif self.class_choice == \"cycle\":\n",
    "            class_ = self.current_class\n",
    "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
    "        elif self.class_choice == \"least_sampled\":\n",
    "            min_count = self.counts[0]\n",
    "            min_classes = [0]\n",
    "            for class_ in range(1, self.num_classes):\n",
    "                if self.counts[class_] < min_count:\n",
    "                    min_count = self.counts[class_]\n",
    "                    min_classes = [class_]\n",
    "                if self.counts[class_] == min_count:\n",
    "                    min_classes.append(class_)\n",
    "            class_ = np.random.choice(min_classes)\n",
    "        return class_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaf9651e-e42f-4b43-a52c-54d8b6fb58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from einops import rearrange\n",
    "from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2f4247e-f74d-4323-8005-242038d992a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":2023,\n",
    "    \"model_name\":\"facebook/timesformer-base-finetuned-k400\",\n",
    "    \"batch_size\":3,\n",
    "    \"learning_rate\":1e-5,\n",
    "    \"data_dir\":'C:/Users/USER/Desktop/python/competition/CarCrashAnalysisAIContest/data',\n",
    "    \"checkpoint_dir\":'./checkpoint',\n",
    "    \"submission_dir\":'./submission',\n",
    "    \"n_classes\":(2,3,4,3),\n",
    "    \"label_dict\":{\n",
    "        -1:[-1,-1,-1,-1],\n",
    "        0:[0,0,0,0],\n",
    "        1:[1,1,1,1],\n",
    "        2:[1,1,1,2],\n",
    "        3:[1,1,2,1],\n",
    "        4:[1,1,2,2],\n",
    "        5:[1,1,3,1],\n",
    "        6:[1,1,3,2],\n",
    "        7:[1,2,1,1],\n",
    "        8:[1,2,1,2],\n",
    "        9:[1,2,2,1],\n",
    "        10:[1,2,2,2],\n",
    "        11:[1,2,3,1],\n",
    "        12:[1,2,3,2]\n",
    "    },\n",
    "    \"label_reverse_dict\":{\n",
    "        (0,0,0,0):0,\n",
    "        (1,1,1,1):1,\n",
    "        (1,1,1,2):2,\n",
    "        (1,1,2,1):3,\n",
    "        (1,1,2,2):4,\n",
    "        (1,1,3,1):5,\n",
    "        (1,1,3,2):6,\n",
    "        (1,2,1,1):7,\n",
    "        (1,2,1,2):8,\n",
    "        (1,2,2,1):9,\n",
    "        (1,2,2,2):10,\n",
    "        (1,2,3,1):11,\n",
    "        (1,2,3,2):12,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7430352-4e4d-414b-a180-ec554b3c9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a1692b-89ab-4739-a668-a191d43d85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{config['data_dir']}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04928136-a408-4278-acbb-6284073c853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sample_id'] = train_df['sample_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "test_df['sample_id'] = test_df['sample_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a728f070-8d8f-4961-9258-bb482aea315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['video_path'] = train_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0403f478-2bb1-4ab6-8589-86d1ffc8e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label']=-1\n",
    "test_df['label_split'] = test_df['label'].apply(config['label_dict'].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba29cd0-c1da-43ab-a68c-b09688c70576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label_split'] = train_df['label'].apply(config['label_dict'].get)\n",
    "train_label_split = np.array(train_df['label_split'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eac6350-eef7-4f10-b9e1-65ed21fd00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_multi_hot = np.hstack([np.eye(n_class, dtype=np.int32)[train_label_split[:,idx]] for idx, n_class in enumerate(config['n_classes'])])\n",
    "train_df['label_multi_hot'] = train_label_multi_hot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be86239-8eb6-4a4d-9b7c-08e5ae197ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_for_dataset, _ , val_df_for_dataset, _  = iterative_train_test_split(X=train_df.values, y=train_label_multi_hot, test_size=0.2)\n",
    "test_df_for_dataset = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b69d041-f0d9-4ba9-94df-e70aedb58319",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_hot_for_sampler = np.array(train_df_for_dataset[:,4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f4029b-f6e6-4b17-82c3-fb898fcec84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df_for_dataset, transform=None):\n",
    "        self.sample_id = df_for_dataset[:,0]\n",
    "        self.video_path = df_for_dataset[:,1]\n",
    "        self.label = df_for_dataset[:,2]\n",
    "        self.label_split = np.array(df_for_dataset[:,3].tolist())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_id[idx]\n",
    "        video_path = self.video_path[idx]\n",
    "        vr = VideoReader(video_path)\n",
    "        video = torch.from_numpy(vr.get_batch(range(50)).asnumpy())\n",
    "        video = rearrange(video, 't h w c -> c t h w')\n",
    "        label = self.label[idx]\n",
    "        label_split = self.label_split[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        video = rearrange(video, 'c t h w -> t c h w')\n",
    "\n",
    "        sample = {\n",
    "            'sample_id':sample_id,\n",
    "            'video':video,\n",
    "            'label':label,\n",
    "            'label_split':label_split\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d84d5626-6b8b-4ea9-81a8-3320337d93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 412/412 [00:00<00:00, 206kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(config['model_name'])\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7612a650-4c4d-4f1a-a7f7-11c22ff6b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = create_video_transform(\n",
    "    mode='train',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")\n",
    "\n",
    "val_transform = create_video_transform(\n",
    "    mode='val',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d1b230d-4ca8-43ce-b7ea-0003c8352f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(train_df_for_dataset, transform=train_transform)\n",
    "val_dataset = VideoDataset(val_df_for_dataset, transform=val_transform)\n",
    "test_dataset = VideoDataset(test_df_for_dataset, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b25026-b607-4f60-a500-49d1ee93fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = MultilabelBalancedRandomSampler(train_multi_hot_for_sampler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= config['batch_size'], sampler=train_sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = config['batch_size']*2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69747543-ce8b-4f0c-aa58-de46c616490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLVideoModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.model = AutoModel.from_pretrained(config['model_name'])\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.LazyLinear(n_class) for n_class in config['n_classes']\n",
    "        ])\n",
    "        self.loss = FocalLoss('multiclass')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x_out = [classifier(x) for classifier in self.classifiers]\n",
    "        return x_out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        loss = sum([self.loss(y_hats[i], batch[\"label_split\"][:,i]) for i in range(len(self.config['n_classes']))])\n",
    "        loss = loss/len(self.config['n_classes'])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = [*y_hats, label]\n",
    "        return step_output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        video, _, _ = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = y_hats\n",
    "        return step_output\n",
    "\n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4, label = [], [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            label.append(step_output[4])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "        label = torch.cat(label).tolist()\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "        \n",
    "        score = f1_score(label,pred, average='macro')\n",
    "        self.log(\"val_score\", score)\n",
    "        return score\n",
    "    \n",
    "    def post_preproc(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4 = [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "\n",
    "        return pred\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Apollo(self.parameters(), lr=self.learning_rate)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbd5dbf9-b7f7-404b-9843-faae40e7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 486M/486M [00:42<00:00, 11.5MB/s] \n",
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: c:\\Users\\USER\\Desktop\\python\\competition\\CarCrashAnalysisAIContest\\CarCrashAnalysisAIContest\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:411: UserWarning: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | TimesformerModel | 121 M \n",
      "1 | classifiers | ModuleList       | 0     \n",
      "2 | loss        | FocalLoss        | 0     \n",
      "-------------------------------------------------\n",
      "121 M     Trainable params\n",
      "0         Non-trainable params\n",
      "121 M     Total params\n",
      "242.518   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 23/810 [03:17<1:52:29,  8.58s/it, loss=0.623, v_num=0]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 6.00 GiB total capacity; 5.07 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3552/1775091058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl_video_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         )\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_to_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_batch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             )\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[1;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;31m# the `batch_idx` is optional with inter-batch parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    368\u001b[0m             )\n\u001b[0;32m    369\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"using_native_amp\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMixedPrecisionPlugin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         self.trainer._call_lightning_module_hook(\n\u001b[0m\u001b[0;32m    371\u001b[0m             \u001b[1;34m\"optimizer_step\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\core\\module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \"\"\"\n\u001b[1;32m-> 1744\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mstep_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         return self.precision_plugin.optimizer_step(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\native_amp.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[1;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;34mf\"Native AMP and the LBFGS optimizer are not compatible (optimizer {optimizer_idx}).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mclosure_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_optimizer_handles_unscaling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mstep_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \"\"\"\n\u001b[0;32m    418\u001b[0m         \u001b[1;31m# manually capture logged metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mtraining_step_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_step\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3552/4226610991.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'video'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_split'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0my_hats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"video\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label_split\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3552/4226610991.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mx_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    621\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 )\n\u001b[0;32m    453\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mspatial_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             spatial_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayernorm_before\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspatial_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mself_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         qkv = (\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 6.00 GiB total capacity; 5.07 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score',\n",
    "    dirpath=config['checkpoint_dir'],\n",
    "    filename=f'{config[\"model_name\"]}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n",
    "    mode='max'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"train_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "pl_video_model = PLVideoModel(config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='auto', \n",
    "    precision=16,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback]\n",
    "                    \n",
    ")\n",
    "trainer.fit(pl_video_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "531d95da-c86f-44e5-9a56-b16505bdc5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\dust\\anaconda3\\envs\\py310_torch1\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\dust\\anaconda3\\envs\\py310_torch1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708d856ea4924f1fa6c11c3600742c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl_video_model_pretrained = PLVideoModel.load_from_checkpoint(\n",
    "    \"./checkpoint/facebook/timesformer-base-finetuned-k400-epoch=08-train_loss=0.1318-val_score=0.5356.ckpt\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto')\n",
    "pred = trainer.predict(pl_video_model_pretrained, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fae171f4-24fe-4428-94e7-e2e920b20b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_post_proc = pl_video_model_pretrained.post_preproc(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53206209-b381-44e9-a27b-815db93db13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(f\"{config['data_dir']}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c14c78f9-1a7a-4a5b-8e32-b768e43ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['label'] = pred_post_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3cc42ee-4233-496b-98df-d7b08f0abe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(f\"{config['submission_dir']}/testsubmit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a2770-efb1-460a-a9be-f5f81c7311d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d5c1156327dacead463cc502c55ebae8ce9c8c01979cf154173ff808e75bf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
