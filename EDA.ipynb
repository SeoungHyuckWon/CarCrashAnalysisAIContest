{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id              video_path  label\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.mp4      7\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.mp4      7\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.mp4      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.mp4      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.mp4      1\n",
       "...          ...                     ...    ...\n",
       "2693  TRAIN_2693  ./train/TRAIN_2693.mp4      3\n",
       "2694  TRAIN_2694  ./train/TRAIN_2694.mp4      5\n",
       "2695  TRAIN_2695  ./train/TRAIN_2695.mp4      0\n",
       "2696  TRAIN_2696  ./train/TRAIN_2696.mp4      0\n",
       "2697  TRAIN_2697  ./train/TRAIN_2697.mp4      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = 'C:/Users/USER/Desktop/python/competition/CarCrashAnalysisAIContest/data/'\n",
    "os.chdir(DIR)\n",
    "os.getcwd()\n",
    "df = pd.read_csv(DIR+'train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train_thumbnail/TRAIN_0000.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train_thumbnail/TRAIN_0001.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train_thumbnail/TRAIN_0002.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train_thumbnail/TRAIN_0003.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train_thumbnail/TRAIN_0004.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>TRAIN_2693</td>\n",
       "      <td>./train_thumbnail/TRAIN_2693.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>TRAIN_2694</td>\n",
       "      <td>./train_thumbnail/TRAIN_2694.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>TRAIN_2695</td>\n",
       "      <td>./train_thumbnail/TRAIN_2695.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>TRAIN_2696</td>\n",
       "      <td>./train_thumbnail/TRAIN_2696.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>TRAIN_2697</td>\n",
       "      <td>./train_thumbnail/TRAIN_2697.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id                        video_path  label\n",
       "0     TRAIN_0000  ./train_thumbnail/TRAIN_0000.jpg      7\n",
       "1     TRAIN_0001  ./train_thumbnail/TRAIN_0001.jpg      7\n",
       "2     TRAIN_0002  ./train_thumbnail/TRAIN_0002.jpg      0\n",
       "3     TRAIN_0003  ./train_thumbnail/TRAIN_0003.jpg      0\n",
       "4     TRAIN_0004  ./train_thumbnail/TRAIN_0004.jpg      1\n",
       "...          ...                               ...    ...\n",
       "2693  TRAIN_2693  ./train_thumbnail/TRAIN_2693.jpg      3\n",
       "2694  TRAIN_2694  ./train_thumbnail/TRAIN_2694.jpg      5\n",
       "2695  TRAIN_2695  ./train_thumbnail/TRAIN_2695.jpg      0\n",
       "2696  TRAIN_2696  ./train_thumbnail/TRAIN_2696.jpg      0\n",
       "2697  TRAIN_2697  ./train_thumbnail/TRAIN_2697.jpg      0\n",
       "\n",
       "[2698 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['video_path'] = df['video_path'].apply(lambda x :  './'+'train_thumbnail'+'/'+x.split('/')[2].split('.')[0]+'.jpg')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Info.\n",
    "\n",
    "13가지의 차량 충돌 상황 Class의 세부 정보\n",
    "\n",
    "crash : 차량 충돌 여부 (No/Yes)<br>\n",
    "ego-Involve : 본인 차량의 충돌 사고 연류 여부 (No/Yes)<br>\n",
    "weather : 날씨 상황 (Normal/Snowy/Rainy)<br>\n",
    "timing : 낮과 밤 (Day/Night)<br>\n",
    "ego-Involve, weather, timing의 정보는 '차량 충돌 사고'가 일어난 경우에만 분석합니다.\n",
    "\n",
    "\n",
    "|crash|ego-Invovle|weather|timing|label|\n",
    "|------|---|---|---|---|\n",
    "|No|-|-|-|0|\n",
    "|Yes|Yes|Normal|Day|1|\n",
    "|Yes|Yes|Normal|Night|2|\n",
    "|Yes|Yes|Snowy|Day|3|\n",
    "|Yes|Yes|Snowy|Night|4|\n",
    "|Yes|Yes|Rainy|Day|5|\n",
    "|Yes|Yes|Rainy|Night|6|\n",
    "|Yes|No|Normal|Day|7|\n",
    "|Yes|No|Normal|Night|8|\n",
    "|Yes|No|Snowy|Day|9|\n",
    "|Yes|No|Snowy|Night|10|\n",
    "|Yes|No|Rainy|Day|11|\n",
    "|Yes|No|Rainy|Night|12|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ego Involve -> 3D ConV<br>\n",
    "weather,timing -> 2D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  label_name\n",
       "0      Nothing       1783\n",
       "1      Day            808\n",
       "2      Night          107\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_timing = df.copy()\n",
    "def change(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    elif x % 2 == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df_timing['label'] = df_timing['label'].apply(change)\n",
    "def change(x):\n",
    "    if x == 0:\n",
    "        return 'Nothing'\n",
    "    elif x == 1:\n",
    "        return 'Day'\n",
    "    else:\n",
    "        return 'Night'\n",
    "df_timing['label_name'] = df_timing['label'].apply(change)\n",
    "df_timing[['label','label_name']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Nothing', 1: 'Day', 2: 'Night'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id,id2label = dict(),dict()\n",
    "for i in df_timing[['label','label_name']].value_counts().items():\n",
    "    label2id[i[0][1]] = i[0][0]\n",
    "    id2label[i[0][0]] = i[0][1]\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \n",
    "# for img_path in df_timing['video_path'].values:\n",
    "#     img = DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {# 10프레임 * 5초\n",
    "    'IMG_SIZE':128,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':4,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor,ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list,label2id,id2label,model_name='google/vit-base-patch16-224-in21k'):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.model_name = model_name\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.image_processor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.get_img(self.img_path_list[index])\n",
    "        inputs = self.image_processor(img, return_tensors=\"pt\")\n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            inputs['label'] = label\n",
    "            return inputs\n",
    "        else:\n",
    "            return inputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def get_img(self, path):\n",
    "        # img = Image.open(path)\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.resize(img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "        # img = img / 255.\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# accuracy metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions=np.argmax(p.predictions, axis=1),\n",
    "        references=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread('./train_thumbnail/TRAIN_0000.jpg')\n",
    "# cv2.imshow('image', test_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = Image.open('./train_thumbnail/TRAIN_0002.jpg')\n",
    "feature_extracter = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "input = feature_extracter(images=test_img,return_tensors='pt')\n",
    "outputs = model(**input).logits\n",
    "\n",
    "outputs.argmax(-1).item()\n",
    "# logits\n",
    "# input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.8588,  0.8667,  0.8667,  ...,  0.4902,  0.5843,  0.5922],\n",
       "          [ 0.8588,  0.8667,  0.8667,  ...,  0.4824,  0.5843,  0.5922],\n",
       "          [ 0.8588,  0.8667,  0.8667,  ...,  0.4667,  0.5686,  0.5765],\n",
       "          ...,\n",
       "          [-0.2549, -0.2549, -0.2706,  ..., -0.5294, -0.5529, -0.5765],\n",
       "          [-0.2549, -0.2549, -0.3020,  ..., -0.5137, -0.5137, -0.5686],\n",
       "          [-0.2471, -0.2706, -0.2941,  ..., -0.5216, -0.5373, -0.6000]],\n",
       "\n",
       "         [[ 0.9765,  0.9843,  0.9843,  ...,  0.6549,  0.7569,  0.7647],\n",
       "          [ 0.9765,  0.9843,  0.9843,  ...,  0.6471,  0.7569,  0.7647],\n",
       "          [ 0.9765,  0.9843,  0.9843,  ...,  0.6314,  0.7412,  0.7490],\n",
       "          ...,\n",
       "          [-0.1529, -0.1529, -0.1765,  ..., -0.4824, -0.4980, -0.5216],\n",
       "          [-0.1529, -0.1529, -0.2078,  ..., -0.4667, -0.4588, -0.5137],\n",
       "          [-0.1451, -0.1686, -0.2078,  ..., -0.4667, -0.4824, -0.5451]],\n",
       "\n",
       "         [[ 0.9373,  0.9451,  0.9451,  ...,  0.6392,  0.7412,  0.7490],\n",
       "          [ 0.9373,  0.9451,  0.9451,  ...,  0.6314,  0.7412,  0.7490],\n",
       "          [ 0.9373,  0.9451,  0.9451,  ...,  0.6157,  0.7255,  0.7333],\n",
       "          ...,\n",
       "          [-0.2863, -0.2863, -0.3098,  ..., -0.6235, -0.6392, -0.6627],\n",
       "          [-0.2863, -0.2863, -0.3333,  ..., -0.6078, -0.6000, -0.6549],\n",
       "          [-0.2784, -0.3020, -0.3333,  ..., -0.6078, -0.6235, -0.6863]]]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(input['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor(\n",
    "        batch['img'],\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # include the labels\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df_timing, df_timing['label'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['video_path'].values, train['label'].values,label2id,id2label)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val['video_path'].values, val['label'].values,label2id,id2label)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # feature_extracter = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for img, labels in tqdm(iter(train_loader)):\n",
    "            # img = feature_extracter(img,return_tensors = 'pt')\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(**img).logits\n",
    "            output = logits.argmax(-1).item()\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, labels in tqdm(iter(val_loader)):\n",
    "            img = feature_extracter(img,return_tensors = 'pt')\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(**img).logits\n",
    "            output = logits.argmax(-1).item()\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += logits.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/540 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_54336/1132254904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthreshold_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'abs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minfer_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_54336/1466614428.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         outputs = self.vit(\n\u001b[0m\u001b[0;32m    788\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    571\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    574\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.eval()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  remove_unused_columns=False,\n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  learning_rate=5e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  gradient_accumulation_steps=4,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  num_train_epochs=2,\n",
    "                                  warmup_ratio=0.1,\n",
    "                                  logging_steps=10,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"accuracy\",\n",
    "                                  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d5c1156327dacead463cc502c55ebae8ce9c8c01979cf154173ff808e75bf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
